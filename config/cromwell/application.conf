# This line is required. It pulls in default overrides from the embedded cromwell `application.conf` needed for proper
# performance of cromwell.
include required(classpath("application"))

akka.http{
    server {
        preview.enable-http2 = on
        request-timeout = 100s
        bind-timeout = 20s
        idle-timeout = 600s
        client.connecting-timeout = 400s
        linger-timeout = 10 min
        max-connections = 2048
        pipelining-limit = 10
        remote-address-attribute = on
        verbose-error-messages = on
        backlog = 1000
        parsing.max-content-length = infinite
    }

    client {

        # The time period within which the TCP connecting process must be completed.
        connecting-timeout = 15s

        # The time after which an idle connection will be automatically closed.
        # Set to `infinite` to completely disable idle timeouts.
        idle-timeout = 120s
    }

     host-connection-pool {
        # The maximum number of parallel connections that a connection pool to a
        # single host endpoint is allowed to establish. Must be greater than zero.
        max-connections = 40
    }
}


languages {
  default: WDL
  WDL {
    versions {
      default: "biscayne"
      "1.0" {
        # WDL draft-3 was our in-progress name for what became WDL 1.0
        language-factory = "languages.wdl.draft3.WdlDraft3LanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
      "biscayne" {
        # WDL biscayne is our in-progress name for what will (probably) become WDL 1.1
        language-factory = "languages.wdl.biscayne.WdlBiscayneLanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
    }
  }
  CWL {
    versions {
      default: "v1.0"
      "v1.0" {
        language-factory = "languages.cwl.CwlV1_0LanguageFactory"
        config {
          strict-validation: false
          enabled: true

          # Optional section to define a command returning paths to output files that
          # can only be known after the user's command has been run
          output-runtime-extractor {
            # Optional docker image on which the command should be run - Must have bash if provided
            docker-image = "stedolan/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09"
            # Single line command executed after the tool has run.
            # It should write to stdout the file paths that are referenced in the cwl.output.json (or any other file path
            # not already defined in the outputs of the tool and therefore unknown when the tool is run)
            command = "cat cwl.output.json 2>/dev/null  jq -r '..  .path? // .location? // empty'"
          }
        }
      }
    }
  }
}

webservice {
  port = 8000
  interface = 0.0.0.0
  binding-timeout = 20s
  instance.name = "reference"
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  # Defaults to false in server mode, and true in run mode.
  # abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  graceful-server-shutdown = true

  file-hash-cache=true

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = false

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 3

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 200

  # Workflows will be grouped by the value of the specified field in their workflow options.
  #
  # Currently, hog-safety will limit concurrent jobs within a hog group:
  # For each hog-group and backend, only (concurrent-job-limit / hog-factor) jobs will be run at any given time.
  #
  # In the future, hog-groups may also apply to other finite resources
  #
  # You can re-use an existing workflow option to allow it to also indicate hog-group,
  # or add a new field created specifically for hog-safety.
  #
  hog-safety {
    # Set this field in the workflow-options file to assign a hog group:
    workflow-option = "hogGroup"

    # Setting to '1' means that a hog group can use the full resources of the Cromwell instance if it needs to:
    hog-factor = 1

    # Time to wait before repeating token log messages - including "queue state" and "group X is a hog" style messages.
    # Leave at 0 to never log these types of message.
    token-log-interval-seconds = 0
  }

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 1

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 50

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  max-scatter-width-per-scatter = 1000000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  total-max-jobs-per-root-workflow = 1000000

  io {
    # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # the quota availble on the GCS API
    number-of-requests = 100000
    per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 8

    # configures exponential backoff of io requests
    backpressure-backoff {
      # starting point
      min = 10 seconds
      # maximum waiting time between attempts
      max = 10 minutes
      # how much longer to wait between each attempt
      multiplier = 2
      # randomizes wait times to avoid large spikes. Must be between 0 and 1.
      randomization-factor = 0.9
    }

    # Amount of time after which an I/O operation will timeout if no response has been received.
    # Note that a timeout may result in a workflow failure so be careful not to set a timeout too low.
    # Unless you start experiencing timeouts under very heavy load there should be no reason to change the default values.
    timeout {
      default = 15 minutes
      # Copy can be a time consuming operation and its timeout can be set separately.
      copy = 1 hour
    }

    gcs {
      parallelism = 10
    }

    nio {
      parallellism = 20
    }
  }

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    lines = 1280000

    bool = 7

    int = 19

    float = 50

    string = 128000

    json = 1280000

    tsv = 1280000

    map = 1280000

    object = 1280000
  }

  # Rate at which Cromwell updates its instrumentation gauge metrics (e.g: Number of workflows running, queued, etc..)
  instrumentation-rate = 5 seconds

  job-rate-control {
    jobs = 25
    per = 1 second
  }

  workflow-heartbeats {
    heartbeat-interval: 2 minutes
    ttl: 10 minutes
    write-batch-size: 10000
    write-threshold: 10000
  }

  job-shell: "/bin/bash"
}

workflow-options {

  # Directory where to write per workflow logs
  workflow-log-dir: "/data/cromwell-workflow-logs"

  # When true, per workflow logs will be deleted after copying
  workflow-log-temporary: false

  # When a workflow type version is not provided on workflow submission, this specifies the default type version.
  workflow-type-version: "development"

  # AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
}

call-caching {
  enabled = true
}

backend {
  default = "Local"
  concurrent-job-limit = 20
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
               concurrent-job-limit = 8
               run-in-background = true
               runtime-attributes = """
             String? docker
             String? docker_user
             String? docker_cpu
             String? docker_memory
             String? docker_swap
             String? docker_network
             String? docker_volume1
             String? docker_volume2
             String? docker_volume3
             String? docker_volume4
             """
               submit = "${job_shell} ${script}"
               submit-docker = """
               # make sure there is no preexisting Docker CID file
               rm -f ${docker_cid}
               # run as in the original configuration without --rm flag (will remove later)
               docker run \
                 --cidfile ${docker_cid} ${"--network="+docker_network} \
                 ${"--cpus=" + docker_cpu} \
                 ${"--memory=" + docker_memory } \
                 ${"--memory-swap=" + docker_swap } \
                 -i \
                 --entrypoint ${job_shell} \
                 -v ${cwd}:${docker_cwd} \
                 --cap-add=SYS_NICE ${"-v "+docker_volume1} ${"-v "+docker_volume2} ${"-v "+docker_volume3} ${"-v "+docker_volume4} \
                 ${docker} ${docker_script}

               # get the return code (working even if the container was detached)
               rc=$(docker wait `cat ${docker_cid}`)

               # remove the container after waiting
               docker rm `cat ${docker_cid}`

               # return exit code
               exit $rc
             """

             kill-docker = "docker kill `cat ${docker_cid}`"

        # Root directory where Cromwell writes job results. This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
        root: "/data/cromwell-executions"

        filesystems {
          local {
            localization: [
              "hard-link", "soft-link", "copy"
            ]

            caching {
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "fingerprint"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            }
          }
        }
      }
    }
  }
}

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    url = "jdbc:mysql://mysql-db/cromwell_db?useSSL=false&rewriteBatchedStatements=true&allowPublicKeyRetrieval=true"
    user = "cromwell"
    password = "cromwell"
    driver = "com.mysql.cj.jdbc.Driver"
    max_connections = 1000000
    allowPublicKeyRetrieval=true
    connectionTimeout = 10000
  }
}
